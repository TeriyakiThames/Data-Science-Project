{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from fuzzywuzzy import process\n",
    "import pycountry\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data Prep: (format country's name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/ASUS/Desktop/Thames' Work/Data Science Project 2024/New CSV/combined_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate rows where multiple countries/instituion are listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แยก row ของเปเปอร์ที่มี country/insitution หลายอัน\n",
    "def separate_countries(row):\n",
    "    countries = row['Country'].split(', ')  \n",
    "    return pd.DataFrame({**row.to_dict(), 'Country': countries}).dropna()\n",
    "\n",
    "df = pd.concat([separate_countries(row) for _, row in df.iterrows()], ignore_index=True)\n",
    "\n",
    "def separate_insitution(row):\n",
    "    countries = row['Institution'].split(', ')  \n",
    "    return pd.DataFrame({**row.to_dict(), 'Institution': countries}).dropna()\n",
    "\n",
    "df = pd.concat([separate_countries(row) for _, row in df.iterrows()], ignore_index=True)\n",
    "df = pd.concat([separate_insitution(row) for _, row in df.iterrows()], ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_country_names = [country.name.lower() for country in pycountry.countries]\n",
    "\n",
    "def clean_country(country_name):\n",
    "    # Clean the input by stripping extra spaces and converting to lowercase\n",
    "    country_name = country_name.strip().lower()\n",
    "    \n",
    "    #use fuzzy matching to find the closest country name from the valid list\n",
    "    if country_name:\n",
    "        match = process.extractOne(country_name, valid_country_names)\n",
    "        if match and match[1] > 80:  # Only accept matches with a score higher than 80 (adjust threshold as needed)\n",
    "            return match[0].title()  # Return the standardized country name with proper capitalization\n",
    "    return \"Unknown\"  \n",
    "\n",
    "df['Country'] = df['Country'].apply(clean_country)\n",
    "df[df['Country'] != \"Unknown\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['Keywords'].dropna().tolist()\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Grid Search to find the most coherence amount of topics\n",
    "def compute_coherence(lda_model, vectorizer):\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    top_words = [topic.argsort()[-10:][::-1] for topic in lda_model.components_]\n",
    "    coherence = sum(\n",
    "        cosine_similarity([lda_model.components_[i]], [lda_model.components_[j]])[0][0]\n",
    "        for i in range(len(top_words)) for j in range(i + 1, len(top_words))\n",
    "    )\n",
    "    return coherence\n",
    "\n",
    "topics_range = range(2, 11)\n",
    "coherence_scores = []\n",
    "\n",
    "for n_topics in topics_range:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    coherence_scores.append(compute_coherence(lda, vectorizer))\n",
    "    print(f\"n_topics={n_topics}, Coherence={coherence_scores[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal n_topics\n",
    "optimal_topics = topics_range[np.argmax(coherence_scores)]\n",
    "lda = LatentDirichletAllocation(n_components=optimal_topics, random_state=42)\n",
    "lda.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/ASUS/Desktop/Thames' Work/Data Science Project 2024/Saved Models/lda_model.pkl\"\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(lda, file)\n",
    "    \n",
    "print(f\"LDA model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = [\n",
    "    \" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:][::-1]])\n",
    "    for topic in lda.components_\n",
    "]\n",
    "topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-topic matrix\n",
    "doc_topic_matrix = lda.transform(doc_term_matrix)\n",
    "topic_df = pd.DataFrame(doc_topic_matrix, columns=topic_names)\n",
    "final_df = pd.concat([df[['Institution', 'Country']], topic_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Insitution Using K Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = optimal_topics\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "final_df['Cluster'] = kmeans.fit_predict(doc_topic_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the K Mean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/ASUS/Desktop/Thames' Work/Data Science Project 2024/Saved Models/kmeans_model.pkl\"\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "    \n",
    "print(f\"KMeans model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#assign cluster name\n",
    "cluster_names = {}\n",
    "\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_docs = final_df[final_df['Cluster'] == cluster].iloc[:, 2:2 + len(topic_names)]  # Adjust column selection\n",
    "\n",
    "    avg_topic_distribution = cluster_docs.mean(axis=0)\n",
    "    top_topic_indices = avg_topic_distribution.argsort()[-3:][::-1]  # Top 3 topics for cluster\n",
    "    top_topic_names = [topic_names[i] for i in top_topic_indices if i < len(topic_names)]\n",
    "    cluster_names[cluster] = \", \".join(top_topic_names)\n",
    "\n",
    "\n",
    "final_df['Cluster Name'] = final_df['Cluster'].apply(lambda x: f\"Cluster {x}: {cluster_names.get(x, 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialization of Each Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Institution, Cluster Name, and Country, then count occurrences\n",
    "country_cluster_counts = final_df.groupby(['Institution', 'Cluster Name', 'Country']).size().reset_index(name='count')\n",
    "\n",
    "# Get the most frequent cluster for each Institution\n",
    "most_frequent_clusters = country_cluster_counts.loc[country_cluster_counts.groupby('Institution')['count'].idxmax()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Specialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
